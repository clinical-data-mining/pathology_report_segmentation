{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the requisite library\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import itertools\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "import intervaltree\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(0, '/mind_data/cdm_repos/cdm-utilities')\n",
    "sys.path.insert(0, '/mind_data/cdm_repos/cdm-utilities/minio_api')\n",
    "sys.path.insert(0, '/mind_data/cdm_repos/pathology_report_segmentation/annotations/')\n",
    "from pathology_extract_pdl1 import PathologyExtractPDL1\n",
    "from minio_api import MinioAPI\n",
    "from utils import set_debug_console, mrn_zero_pad, print_df_without_index, convert_to_int\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug_console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_minio_env = '/mind_data/cdm_repos/cdm-utilities/minio_env.txt'\n",
    "fname_path = 'pathology/table_pathology_clean.tsv'\n",
    "fname_save = 'pathology/pathology_pdl1_calls_cf.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_minio = MinioAPI(fname_minio_env=fname_minio_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dev Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj = obj_minio.load_obj(path_object=fname_path)\n",
    "# df_path1 = pd.read_csv(obj, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_drop = ['DTE_PATH_PROCEDURE',\n",
    "#              'PATH_RPT_ID',\n",
    "#              'ASSOCIATED_PATH_REPORT_ID',\n",
    "#              'DMP_ID',\n",
    "#              'SAMPLE_ID',\n",
    "#              'SPECIMEN_SUBMISSION_LIST',\n",
    "#              'PATH_REPORT_TYPE_GENERAL',\n",
    "#              'RPT_CHAR_LEN']\n",
    "# df_path = df_path1.drop(columns=cols_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # s = r'PD[-]*L[-]*1[ ]*[:(]*'\n",
    "# s = r'pd[-]*l[-]*1'\n",
    "# df_path['PATH_REPORT_NOTE'] = df_path['PATH_REPORT_NOTE'].str.lower()\n",
    "# has_pdl1 = df_path['PATH_REPORT_NOTE'].str.contains(s, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reg = re.compile(s)\n",
    "# get_spans = lambda x: [r.span() for r in reg.finditer(x)]\n",
    "# tuple_to_list = lambda y: [item for t in y for item in t]\n",
    "# df_sub = df_path.loc[has_pdl1]\n",
    "# # t = df_sub['PATH_REPORT_NOTE'].apply(lambda z: tuple_to_list(get_spans(z)))\n",
    "# series_spans = df_sub['PATH_REPORT_NOTE'].apply(lambda z: get_spans(z))\n",
    "# series_span_list = series_spans.apply(lambda x: tuple_to_list(x))\n",
    "\n",
    "# # tt = t.apply(lambda x: (min(x), max(x)+200)).rename('SPAN')\n",
    "# # df_sub = pd.concat([df_sub, tt], axis=1, sort=False)\n",
    "# # # out = [item for t in lt for item in t]\n",
    "# # text = df_sub.apply(lambda x: x['PATH_REPORT_NOTE'][x['SPAN'][0]:x['SPAN'][1]].lower(), axis=1).rename('PDL1_TEXT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 312\n",
    "# print(series_span_list.iloc[p])\n",
    "# dist = [(series_span_list.iloc[p][i+1] - series_span_list.iloc[p][i]) for i,ind in enumerate(series_span_list.iloc[p][:-1]) if i % 2 == 1]\n",
    "# dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o = [(x[0], x[1]+200) for x in series_spans.iloc[p]]\n",
    "# tree = intervaltree.IntervalTree.from_tuples(o)\n",
    "# print(tree)\n",
    "# tree.merge_overlaps()\n",
    "# print(tree)\n",
    "\n",
    "# l = [[*range(x[0], x[1])] for x in tree.iter()]\n",
    "\n",
    "# index_text = [item for sublist in l for item in sublist]\n",
    "# o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_ind = lambda y: [(x[0], x[1]+200) for x in y]\n",
    "# o = series_spans.apply(lambda x: fix_ind(x))\n",
    "# trees = [intervaltree.IntervalTree.from_tuples(x) for x in o]\n",
    "# trees_overlap = [tree.merge_overlaps() for tree in trees]\n",
    "# tree_ranges = [[range(x[0], x[1]) for x in z.iter()] for z in trees]\n",
    "# # \n",
    "# # .merge_overlaps()\n",
    "# # tree = intervaltree.IntervalTree.from_tuples(o)\n",
    "# # print(tree)\n",
    "# # tree.merge_overlaps()\n",
    "# # print(tree)\n",
    "\n",
    "# # l = [[*range(x[0], x[1])] for x in tree.iter()]\n",
    "\n",
    "# # index_text = [item for sublist in l for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_list_merged = pd.Series(tree_ranges, index=o.index, name='TEXT_INDICES')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub1 = pd.concat([df_sub, series_list_merged], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT_PDL1 = df_sub1.apply(lambda z: ''.join([z['PATH_REPORT_NOTE'][(z['TEXT_INDICES'][i][0]):(z['TEXT_INDICES'][i][-1])] for i,ind_range in enumerate(z['TEXT_INDICES'])]), axis=1).rename('TEXT_PDL1').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub2 = pd.concat([df_sub1, TEXT_PDL1], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_rpt = 1\n",
    "# s = text_full.iloc[index_rpt]\n",
    "# # [(tree_ranges[index_rpt][1][0]):(tree_ranges[index_rpt][1][-1])]\n",
    "# ''.join([text_full.iloc[index_rpt][(tree_ranges[index_rpt][i][0]):(tree_ranges[index_rpt][i][-1])] for i,ind_range in enumerate(tree_ranges[index_rpt])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = df_sub1['PATH_REPORT_NOTE'].iloc[0].lower()\n",
    "# string[[12, 212]]\n",
    "# ''.join([letter for idx, letter in enumerate(string) if idx in  df_sub1['TEXT_INDICES'].iloc[0]])\n",
    "# df_sub1.apply(lambda x: ''.join([letter for idx, letter in enumerate(x['PATH_REPORT_NOTE'].lower()) if idx in  x['TEXT_INDICES']]), axis=1)\n",
    "# ''.join([letter for idx, letter in enumerate(s) if not any(idx in rng for rng in ranges)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sub.apply(lambda x: x['PATH_REPORT_NOTE'].lower(), axis=1).rename('PDL1_TEXT')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [(series_spans.iloc[p][i+1] - series_spans.iloc[p][i]) for i,ind in enumerate(series_span_list.iloc[p][:-1]) if i % 2 == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text.iloc[p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(df_path.loc[has_pdl1].sample()['PATH_REPORT_NOTE'].iloc[0])\n",
    "# # df_path.loc[has_pdl1, 'PATH_REPORT_TYPE'].value_counts()\n",
    "# cols_logic = ['HAS_PDL1_POS',\n",
    "#         'HAS_PDL1_NEG',\n",
    "#         'HAS_PDL1_PERC',\n",
    "#         'HAS_PDL1_CPS_1',\n",
    "#         'HAS_PDL1_CPS_2',\n",
    "#         'HAS_PDL1_TPS_1',\n",
    "#         'HAS_PDL1_TPS_2',\n",
    "#         'HAS_PDL1_IPS'\n",
    "#        ]\n",
    "#\n",
    "# pattern_neg = re.compile(\"(?:no[\\s]*labeling[\\s]*is[\\s]*seen[\\s]*in[\\s]*tumor[\\s]*cells|negative)\")\n",
    "# pattern_cps = re.compile(\"combined[\\s]*positiv[eity]+[\\s]*score\")\n",
    "# pattern_tps = re.compile(\"tumor[\\s]*proportion[\\s]*score\")\n",
    "# pattern_tps_2 = re.compile(\"[percentagecontribution]+[\\s]*of[\\s]*tumor[\\s]*cells\")\n",
    "# pattern_ips = re.compile(\"[percentagecontribution]+[\\s]*of[\\s]*[inflammatoryimmune]+\")\n",
    "# pattern_pos = re.compile(\"positive \\(cps >=1|positive \\(>=1\")\n",
    "# # t = df_path.loc[has_pdl1, 'PATH_REPORT_NOTE'].apply(lambda x: re.split(s, x)[1:])\n",
    "# text = df_path.loc[has_pdl1, 'PATH_REPORT_NOTE'].apply(lambda x: ''.join(['pd-l1 ' + y[:200] for y in re.split(s, x)[1:]])).rename('PDL1_TEXT')\n",
    "# # ttt = tt.apply(lambda x: ''.join(['pd-l1 ' + x for x in x]))\n",
    "# # ttt = tt.apply(lambda x: [(re.search(pattern_pos, y) is not None, \\\n",
    "# #                            re.search(pattern_neg, y) is not None, \\\n",
    "# #                            '%' in y, \\\n",
    "# #                            'of 100' in y, re.search(pattern_cps, y) is not None, re.search(pattern_tps, y) is not None, re.search(pattern_tps_2, y) is not None,\n",
    "# #                            re.search(pattern_ips, y) is not None\n",
    "# #                           )\n",
    "# #                           for y in x])\n",
    "#\n",
    "# logic_available = text.apply(lambda y: [re.search(pattern_pos, y) is not None, \\\n",
    "#                            re.search(pattern_neg, y) is not None, \\\n",
    "#                            '%' in y, \\\n",
    "#                            'of 100' in y, re.search(pattern_cps, y) is not None, re.search(pattern_tps, y) is not None, re.search(pattern_tps_2, y) is not None,\n",
    "#                            re.search(pattern_ips, y) is not None\n",
    "#                                 ] )\n",
    "#\n",
    "# df_available = pd.DataFrame(logic_available.tolist(), index= logic_available.index, columns=cols_logic)\n",
    "#\n",
    "# df_path = df_path.assign(MENTIONS_PDL1=has_pdl1)\n",
    "# df_path_f = pd.concat([df_path, text, df_available], axis=1, sort=False)\n",
    "# df_path_f[cols_logic] = df_path_f[cols_logic].fillna(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(t.sample().iloc[0])\n",
    "# fadfa= 200\n",
    "# t.iloc[0][:fadfa]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _add_anno_template(df, regex_pattern, col_text, col_name, col_filter=None):\n",
    "#     a = df[df[col_filter] == True]\n",
    "#     pattern = re.compile(regex_pattern, re.IGNORECASE)\n",
    "#     pdl1_anno = a[col_text].apply(lambda z: re.findall(regex_pattern, z)).rename(col_name)\n",
    "#\n",
    "#     df = pd.concat([df, pdl1_anno], axis=1, sort=False)\n",
    "#\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_text = 'PDL1_TEXT'\n",
    "# col_name = 'PDl1_PERCENTAGE'\n",
    "# col_filter = 'HAS_PDL1_PERC'\n",
    "# regex = \"([<>=]*[ ]*[\\d+\\.\\d+-]*\\d+[\\.\\d+]*?[ ]*%)\"\n",
    "#\n",
    "#\n",
    "# df_path_f2 = _add_anno_template(df=df_path_f,\n",
    "#                                 regex_pattern=regex,\n",
    "#                                 col_text=col_text,\n",
    "#                                 col_name=col_name,\n",
    "#                                 col_filter=col_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path_f2.loc[df_path_f2[col_filter], col_name].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDL_POS = ttt.apply(lambda x: any([y[0] for y in x])).rename('HAS_PDL1_POS')\n",
    "# PDL_NEG = ttt.apply(lambda x: any([y[1] for y in x])).rename('HAS_PDL1_NEG')\n",
    "# PDL_PERC = ttt.apply(lambda x: any([y[2] for y in x])).rename('HAS_PDL1_PERC')\n",
    "# PDL_HAS_CPS_1 = ttt.apply(lambda x: any([y[3] for y in x])).rename('HAS_PDL1_CPS_1')\n",
    "# PDL_HAS_CPS_2 = ttt.apply(lambda x: any([y[4] for y in x])).rename('HAS_PDL1_CPS_2')\n",
    "# PDL_HAS_TPS_1 = ttt.apply(lambda x: any([y[5] for y in x])).rename('HAS_PDL1_TPS_1')\n",
    "# PDL_HAS_TPS_2 = ttt.apply(lambda x: any([y[6] for y in x])).rename('HAS_PDL1_TPS_2')\n",
    "# PDL_HAS_IPS = ttt.apply(lambda x: any([y[7] for y in x])).rename('HAS_PDL1_IPS')\n",
    "# PDL_HAS_IPS.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdl1_perc_logic = ttt.apply(lambda x: [y[2] for y in x]).rename('PDL1_TEXT_LOGIC')\n",
    "# pdl1_perc_logic_2 = ttt.apply(lambda x: [y[3] for y in x]).rename('PDL1_TEXT_LOGIC_2')\n",
    "# pdl1_perc_logic_3 = ttt.apply(lambda x: [y[4] for y in x]).rename('PDL1_TEXT_LOGIC_3')\n",
    "# pdl1_perc_logic_4 = ttt.apply(lambda x: [y[5] for y in x]).rename('PDL1_TEXT_LOGIC_4')\n",
    "# pdl1_perc_logic_5 = ttt.apply(lambda x: [y[6] for y in x]).rename('PDL1_TEXT_LOGIC_5')\n",
    "# pdl1_perc_logic_6 = ttt.apply(lambda x: [y[7] for y in x]).rename('PDL1_TEXT_LOGIC_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path = df_path.assign(MENTIONS_PDL1=has_pdl1)\n",
    "# ttt.apply(lambda x: [y[2] for y in x])\n",
    "# df_path_f = pd.concat([df_path, PDL_POS, PDL_NEG, PDL_PERC, PDL_HAS_CPS_1, PDL_HAS_CPS_2, PDL_HAS_TPS_1, PDL_HAS_TPS_2, PDL_HAS_IPS, pdl1_perc_logic, pdl1_perc_logic_2, pdl1_perc_logic_3, pdl1_perc_logic_4, pdl1_perc_logic_5, pdl1_perc_logic_6, tt], axis=1, sort=False)\n",
    "# list_fill_na = ['HAS_PDL1_POS', 'HAS_PDL1_NEG', 'HAS_PDL1_PERC', 'HAS_PDL1_CPS_1', 'HAS_PDL1_CPS_2', 'HAS_PDL1_TPS_1', 'HAS_PDL1_TPS_2', 'HAS_PDL1_IPS']\n",
    "# df_path_f[list_fill_na] = df_path_f[list_fill_na].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perc(row):\n",
    "#     pattern = re.compile(\"([<>=]*[ ]*[\\d+\\.\\d+-]*\\d+[\\.\\d+]*?[ ]*%)\",re.IGNORECASE)\n",
    "#     log_list = row['PDL1_TEXT_LOGIC']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#     # filt_text_2 = [x.group(1) for x in filt_text if x is not None]\n",
    "#\n",
    "#     return filt_text\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = df_path_f[df_path_f['HAS_PDL1_PERC']]\n",
    "# pdl1_perc = a.apply(lambda x: perc(x), axis=1).rename('PDl1_PERCENTAGE')\n",
    "# df_path_f2 = pd.concat([df_path_f, pdl1_perc], axis=1, sort=False)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cps_extract_1(row):\n",
    "#     pattern = re.compile(\"([<>=]*\\d+[\\.\\d]*?)(?=\\s*\\D*of 100)\",re.IGNORECASE)\n",
    "#     log_list = row['PDL1_TEXT_LOGIC_2']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#     # filt_text_2 = [x.group() for x in filt_text if x is not None]\n",
    "#\n",
    "#     return filt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = df_path_f2[df_path_f2['HAS_PDL1_CPS_1']]\n",
    "# pdl1_perc_2 = b.apply(lambda x: cps_extract_1(x), axis=1).rename('PDl1_CPS_1')\n",
    "# df_path_f3 = pd.concat([df_path_f2, pdl1_perc_2], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cps_extract_2(row):\n",
    "#     pattern = re.compile(\"combined[\\s]*positiv[eity]+[\\s]*score[^><=0-9]*([<>=]*\\d+[\\.\\d+]*[/]*[\\d]*)\",re.IGNORECASE)\n",
    "#     log_list = row['PDL1_TEXT_LOGIC_3']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#\n",
    "#     return filt_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = df_path_f3[(df_path_f3['HAS_PDL1_CPS_2'] == True)]\n",
    "# pdl1_cps = c.apply(lambda x: cps_extract_2(x), axis=1).rename('PDl1_CPS_2')\n",
    "# df_path_f4 = pd.concat([df_path_f3, pdl1_cps], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tps_extract_1(row):\n",
    "#     pattern = re.compile(\"tumor[\\s]*proportion[\\s]*score[^><=0-9]*([<>=]*\\d+[\\.\\d+]*[/]*[\\d]*)\",re.IGNORECASE)\n",
    "#     log_list = row['PDL1_TEXT_LOGIC_4']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#\n",
    "#     return filt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = df_path_f4[(df_path_f4['HAS_PDL1_TPS_1'] == True)]\n",
    "# pdl1_tps = d.apply(lambda x: tps_extract_1(x), axis=1).rename('PDl1_TPS_1')\n",
    "# df_path_f5 = pd.concat([df_path_f4, pdl1_tps], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tps_extract_2(row):\n",
    "#     pattern = re.compile(\"[percentagecontribution]+[\\s]*of[\\s]*tumor[\\s]*cells[^><=0-9]*([<>=]*\\d+[\\.\\d+]*[/]*[\\d]*)\",re.IGNORECASE) # TODO add \"percentage of tumor cells\"\n",
    "#     log_list = row['PDL1_TEXT_LOGIC_5']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#\n",
    "#     return filt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = df_path_f5[(df_path_f5['HAS_PDL1_TPS_2'] == True)]\n",
    "# pdl1_tps_2 = e.apply(lambda x: tps_extract_2(x), axis=1).rename('PDl1_TPS_2')\n",
    "# df_path_f6 = pd.concat([df_path_f5, pdl1_tps_2], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ips_extract(row):\n",
    "#     pattern = re.compile(\"[percentagecontribution]+[\\s]*of[\\s]*inflammatory[^><=0-9]*([<>=]*\\d+[\\.\\d+]*[/]*[\\d]*)\",re.IGNORECASE) # TODO add \"percentage of immune cells\"\n",
    "#     log_list = row['PDL1_TEXT_LOGIC_6']\n",
    "#     pdl1_text = row['PDL1_TEXT']\n",
    "#     filt_text = [re.findall(pattern, z) for i,z in enumerate(pdl1_text) if log_list[i] == True]\n",
    "#\n",
    "#     return filt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = df_path_f6[(df_path_f6['HAS_PDL1_IPS'] == True)]\n",
    "# pdl1_ips = f.apply(lambda x: ips_extract(x), axis=1).rename('PDl1_IPS')\n",
    "# df_path_f7 = pd.concat([df_path_f6, pdl1_ips], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_pdl1_f = df_path_f7[['MRN', 'ACCESSION_NUMBER', 'MENTIONS_PDL1', 'PDL1_TEXT', 'HAS_PDL1_POS', 'HAS_PDL1_NEG', 'PDl1_PERCENTAGE', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'PDl1_IPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 = (df_pdl1_f2['MENTIONS_PDL1'] == True)\n",
    "# l2 = (df_pdl1_f2['HAS_PDL1_POS'] == False)\n",
    "# l3 = (df_pdl1_f2['HAS_PDL1_NEG'] == False)\n",
    "# l4 = (df_pdl1_f2['PDl1_PERCENTAGE'].isnull())\n",
    "# l5 = (df_pdl1_f2['PDl1_CPS_1'].isnull())\n",
    "# l6 = (df_pdl1_f2['PDl1_CPS_2'].isnull())\n",
    "# l7 = (df_pdl1_f2['PDl1_TPS_1'].isnull())\n",
    "# l8 = (df_pdl1_f2['PDl1_TPS_2'].isnull())\n",
    "# l10 = (df_pdl1_f2['PDl1_IPS'].isnull())\n",
    "# l9 = (df_pdl1_f2['PATH_REPORT_TYPE'].str.contains('Surg'))\n",
    "# logic_missing = l1 & l2 & l3 & l4 & l5 & l6 & l7 & l8 & l9 & l10\n",
    "# df_pdl1_f2 = df_pdl1_f2.assign(DEBUG_REQUIRED=logic_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_path_f7;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pdl1_f.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load created file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = obj_minio.load_obj(path_object=fname_save)\n",
    "df_pdl1 = pd.read_csv(obj, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1['MENTIONS_PDL1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up columns with lists"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_empty_lists(df, col):\n",
    "    df1 = df.loc[df[col].notnull(), col]\n",
    "    len_cps1 = df1.apply(lambda x: len(x))\n",
    "    df1.loc[len_cps1 == 0] = np.NaN\n",
    "\n",
    "    df[col] = df1\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def select_item_in_list(df, col):\n",
    "    # TODO Come up with better scheme than picking the first item\n",
    "    cps1 = df.loc[df[col].notnull(), col].apply(lambda x: x[0])\n",
    "    df.loc[cps1.index, col] = cps1.values\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_numeric_strings(df, col):\n",
    "    cps1 = df.loc[df[col].notnull(), col]\n",
    "    cps1.loc[cps1.str.contains('<1', '<0.')] = 0\n",
    "    cps1[cps1.str.contains('|'.join(['>1', '>=1'])).fillna(False)] = 1\n",
    "    cps1[cps1.str.contains('|'.join(['>','<','='])).fillna(False)].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x))\n",
    "    cps1_rep = cps1[cps1.str.contains('|'.join(['>','<','='])).fillna(False)].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x))\n",
    "    cps1_rep2 = cps1[cps1.str.contains('/100').fillna(False)].apply(lambda x: re.sub('/100', '', x))\n",
    "    cps1.loc[cps1_rep.index] = cps1_rep.values\n",
    "    cps1.loc[cps1_rep2.index] = cps1_rep2.values\n",
    "    cps1 = pd.to_numeric(cps1, errors='coerce')\n",
    "\n",
    "    df.loc[cps1.index, col] = cps1.values\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o = df_pdl1.copy()\n",
    "list_columns = ['PDl1_PERCENTAGE', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'PDl1_IPS']\n",
    "for col in list_columns:\n",
    "    df_pdl1o[col] = df_pdl1o[col].apply(lambda x: eval(x)  if type(x) == str else x)\n",
    "    df_pdl1o = remove_empty_lists(df=df_pdl1o, col=col)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_columns_pick_one = ['PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'PDl1_IPS']\n",
    "for col in list_columns_pick_one:\n",
    "    print(col)\n",
    "    df_pdl1o = select_item_in_list(df=df_pdl1o, col=col)\n",
    "    df_pdl1o = clean_numeric_strings(df=df_pdl1o, col=col)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1['PDl1_CPS_2'].notnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rules for PD-L1 Positivity\n",
    "- Clean variables\n",
    "    - Combine CPS\n",
    "    - Combine TPS\n",
    "    - Look to percentages\n",
    "        - If there is only (1) percentage in list, use as TPS\n",
    "        - If there are (2) percentages in list, use first as TPS and second as IPS\n",
    "        - If there are more than (2) percentages in list, ignore, for now..\n",
    "\n",
    "- Use markers for CPS\n",
    "- If unavailable, use TPS values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine and clean CPS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cps1 = df_pdl1o.loc[df_pdl1o['PDl1_CPS_1'].notnull(), 'PDl1_CPS_1']\n",
    "cps2 = df_pdl1o.loc[df_pdl1o['PDl1_CPS_2'].notnull(), 'PDl1_CPS_2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_CPS'] = cps2\n",
    "df_pdl1o['PDl1_CPS'] = df_pdl1o['PDl1_CPS'].fillna(cps1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_CPS'].notnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine and clean TPS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tps1 = df_pdl1o.loc[df_pdl1o['PDl1_TPS_1'].notnull(), 'PDl1_TPS_1']\n",
    "tps2 = df_pdl1o.loc[df_pdl1o['PDl1_TPS_2'].notnull(), 'PDl1_TPS_2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_TPS'] = tps2\n",
    "df_pdl1o['PDl1_TPS'] = df_pdl1o['PDl1_TPS'].fillna(tps1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_TPS'].notnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combine Percentages\n",
    "#### Get length of list for percentages\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_perc = df_pdl1o.loc[df_pdl1o['PDl1_PERCENTAGE'].notnull()].copy()\n",
    "list_len = df_perc['PDl1_PERCENTAGE'].apply(lambda x: len(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Take first percentage in the list and assume that is the TPS score of the PD-L1 test\n",
    "TODO: This is NOT an ideal way of doing this, and will need to rework this down the road"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# perc_single = df_perc.loc[list_len == 1, 'PDl1_PERCENTAGE'].apply(lambda x: x[0]).str.strip()\n",
    "perc_single = df_perc['PDl1_PERCENTAGE'].apply(lambda x: x[0]).str.strip()\n",
    "perc_single = perc_single.apply(lambda x: re.sub('[% ]*', '', x))\n",
    "perc_single = perc_single.str.replace('|'.join(['>1', '>=1']), '1')\n",
    "perc_single = perc_single.str.replace('|'.join(['<0.', '<1']), '0')\n",
    "\n",
    "# Handle ranges by taking higher range\n",
    "dashes = perc_single.apply(lambda x: re.search(\"[-]+(\\d)+\", x))\n",
    "perc_clean1 = dashes[dashes.notnull()].apply(lambda x: x[0][1:])\n",
    "perc_single.loc[perc_clean1.index] = perc_clean1.values\n",
    "\n",
    "# For the remaining, remove special characters\n",
    "perc_single = perc_single.apply(lambda x: re.sub('[><=-]*', '', x))\n",
    "perc_single = pd.to_numeric(perc_single, errors='coerce')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_TPS'] = df_pdl1o['PDl1_TPS'].fillna(perc_single)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o['PDl1_TPS'].notnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1o"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Handle cases with 2 percentages extracted\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_pdl1o.loc[(df_pdl1o['PDl1_TPS'].isnull() & df_pdl1o['PDl1_CPS'].isnull()) & df_pdl1o['PDl1_PERCENTAGE'].notnull(), ['PDL1_TEXT', 'PDl1_PERCENTAGE']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1_f[df_pdl1_f['PDL1_POSITIVE'].isnull()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Final cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_final = ['MRN', 'ACCESSION_NUMBER', 'MENTIONS_PDL1', 'PDL1_TEXT',\n",
    "              'E1L3N', 'SP-142', 'SP-263', 'CD-2764', '22C3', 'PDl1_PERCENTAGE',\n",
    "              'HAS_PDL1_POS', 'HAS_PDL1_NEG', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'DEBUG_REQUIRED', 'PDl1_CPS', 'PDl1_TPS', 'PDl1_IPS']\n",
    "# cols_final = ['PDL1_TEXT',\n",
    "#               'E1L3N', 'SP-142', 'SP-263', 'CD-2764', '22C3', 'PDl1_PERCENTAGE',\n",
    "#               'HAS_PDL1_POS', 'HAS_PDL1_NEG', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'DEBUG_REQUIRED', 'PDl1_CPS', 'PDl1_TPS', 'PDl1_IPS']\n",
    "df_pdl1_f = df_pdl1o[cols_final]\n",
    "\n",
    "df_pdl1_f = df_pdl1_f.assign(PDL1_POSITIVE=np.NaN)\n",
    "df_pdl1_f.loc[df_pdl1_f['HAS_PDL1_NEG'] == True, 'PDL1_POSITIVE'] = False\n",
    "df_pdl1_f.loc[(df_pdl1_f['PDl1_CPS'] >= 10), 'PDL1_POSITIVE'] = True\n",
    "df_pdl1_f.loc[(df_pdl1_f['PDl1_TPS'] >= 1), 'PDL1_POSITIVE'] = True\n",
    "df_pdl1_f.loc[df_pdl1_f['HAS_PDL1_POS'] == True, 'PDL1_POSITIVE'] = True\n",
    "df_pdl1_f.loc[df_pdl1_f['PDl1_CPS'] < 10, 'PDL1_POSITIVE'] = False\n",
    "df_pdl1_f.loc[df_pdl1_f['PDl1_TPS'] == 0, 'PDL1_POSITIVE'] = False\n",
    "\n",
    "# df_pdl1_f[((df_pdl1_f['PDl1_CPS'] > 10) | (df_pdl1_f['PDl1_TPS'] > 1) | (df_pdl1_f['HAS_PDL1_POS'] == True)) & (df_pdl1_f['HAS_PDL1_NEG'] == True)].shape\n",
    "# print(g['PDL1_TEXT'].iloc[0])\n",
    "# print(g['PDl1_PERCENTAGE'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save final cleaned PD-L1 data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obj_minio.save_obj(df=df_pdl1_f, path_object=fname_save, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test out\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pdl1_f[(df_pdl1_f['HAS_PDL1_NEG'] == True) & (df_pdl1_f['PDL1_POSITIVE'] == True)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = (df_path_f7['MENTIONS_PDL1'] == True)\n",
    "l2 = (df_path_f7['HAS_PDL1_POS'] == False)\n",
    "l3 = (df_path_f7['HAS_PDL1_NEG'] == False)\n",
    "l4 = (df_path_f7['HAS_PDL1_PERC'] == False) \n",
    "l5 = (df_path_f7['HAS_PDL1_CPS_1'] == False)\n",
    "l6 = (df_path_f7['HAS_PDL1_CPS_2'] == False)\n",
    "l7 = (df_path_f7['HAS_PDL1_TPS_1'] == False)\n",
    "l8 = (df_path_f7['HAS_PDL1_TPS_2'] == False)\n",
    "l9 = (df_path_f7['PATH_REPORT_TYPE'].str.contains('Surg'))\n",
    "\n",
    "missing = df_path_f7[l1 & l2 & l3 & l4 & l5 & l6 & l7 & l8 & l9]\n",
    "missing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing['PATH_REPORT_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = missing.sample()\n",
    "print(w['ACCESSION_NUMBER'].iloc[0])\n",
    "print(w['PDL1_TEXT'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df_pdl1_f[(df_pdl1_f['MENTIONS_PDL1'] == True) & ((df_pdl1_f['PDl1_TPS_1'].isnull()) | (df_pdl1_f['PDl1_TPS_2'].isnull()))].sample()\n",
    "print(v['PDL1_TEXT'].iloc[0])\n",
    "v[['MRN', 'ACCESSION_NUMBER','PDL1_TEXT', 'HAS_PDL1_POS', 'HAS_PDL1_NEG', 'PDl1_PERCENTAGE', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'PDl1_IPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_num = 'S18-71966'\n",
    "# print(df_pdl1.loc[df_pdl1['ACCESSION_NUMBER'] == acc_num, 'PATH_REPORT_NOTE'].iloc[0])\n",
    "# df_pdl1[df_pdl1['ACCESSION_NUMBER'] == acc_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_minio.save_obj(df=df_pdl1_f, path_object=fname_save, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_pdl1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = df_pdl1[df_pdl1['MENTIONS_PDL1'] & df_pdl1['HAS_PDL1_POS']].sample()\n",
    "print(w['PDL1_TEXT'].iloc[0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "PD-L1 and mmr proteins will be attempted; an\\r\\naddendum will follow.\\r\\n\\r\\n\\r\\ni attest that the above diagnosis is based upon my personal examination of\\r\\nthe slides (and/or other material), and that i have revie PD-L1 clone e1l3n): positive \\r\\ncombined positive score (cps): 60(of 100); see note\\r\\n\\r\\nnote:\\r\\nthe PD-L1 combined positive score (cps) reflects the aggregate counting of\\r\\n PD-L1 staining in both tumor and adjacent mononuclear inflammatory cells\\r\\n(this approach to PD-L1 evaluation applies to gastric adenocarcinomas\\r\\naccording to current guidelines).\\r\\n\\r\\npercentage of tumor cells staining: 70%\\r\\nrelative contribution of tumor cells to the cps: 85%%\\r\\nrelative contributio PD-L1 clone e1l3n (cell signaling) has been\\r\\nvalidated against clone 22c3 (pharmdx) and found to be comparable.\\r\\n\\r\\nimmunohistochemistry for dna mismatch repair proteins:\\r\\nmsh6: staining present in tumor \\r\\np\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_text = 'PATH_REPORT_NOTE'\n",
    "obj_pdl1 = PathologyExtractPDL1(fname_minio_env, \n",
    "                                 fname=fname_path, \n",
    "                                 col_text=col_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1_f2 = obj_pdl1.return_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1_f2['MRN'].str.contains('nan').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1_f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing2 = df_pdl1_f2[df_pdl1_f2['DEBUG_REQUIRED'] == True]\n",
    "print(missing2.shape)\n",
    "missing2['PATH_REPORT_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdl1_f2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = missing2.sample()\n",
    "print(w['ACCESSION_NUMBER'].iloc[0])\n",
    "print(w['PDL1_TEXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_pdl1_f2[(df_pdl1_f2['MENTIONS_PDL1'] == True) & (df_pdl1_f2['DEBUG_REQUIRED'] == False)]\n",
    "x['PATH_REPORT_TYPE'].value_counts()/x.shape[0]\n",
    "print(x.shape[0])\n",
    "\n",
    "# x.columns\n",
    "# 'HAS_PDL1_POS', 'HAS_PDL1_NEG', 'PDl1_PERCENTAGE', 'PDl1_CPS_1', 'PDl1_CPS_2', 'PDl1_TPS_1', 'PDl1_TPS_2', 'PDl1_IPS'\n",
    "((x['PDl1_PERCENTAGE'].notnull() | x['PDl1_CPS_1'].notnull() | x['PDl1_CPS_2'].notnull() | x['PDl1_TPS_2'].notnull() | x['PDl1_TPS_2'].notnull()) & (x['HAS_PDL1_NEG'] == False)).sum()\n",
    "w = x.sample()\n",
    "print(w['PDL1_TEXT'].iloc[0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x[x['PDl1_CPS_1'].notnull()]\n",
    "w = y[y['PDl1_CPS_1'].apply(lambda x: len(x) > 1)].sample()\n",
    "print(w['PDL1_TEXT_DISPLAY'].iloc[0])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w['PDL1_TEXT'].iloc[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['HAS_PDL1_POS'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 'e1l3n clone) testing\\r\\nby immunohistochemistry. \\r\\ncombined positivity score (cps): 18\\r\\n'\n",
    "pattern = re.compile(\"combined[\\s]*positiv[eity]+[\\s]*score[^><=0-9]*([<>=]*\\d+[\\.\\d+]*[/]*[\\d]*)\",re.IGNORECASE)\n",
    "filt_text = re.findall(pattern, z)\n",
    "filt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Try Spacy for rule based processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = rpt\n",
    "doc = nlp(text)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [r\"PD-L1 \\(\", r\"PD-L1:\", r\"PD-L1|PDL-1|PDL1\"]\n",
    "terms = [\"DIAGNOSIS\"]\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"pdl1\", patterns)\n",
    "\n",
    "\n",
    "matches = matcher(doc)\n",
    "print([(doc[start:end], start, end) for match_id, start, end in matches])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = rpt\n",
    "text = u\"with the following\\nresults:\\nPD-L1 (clone E1L3N): Negative Percentage of tumor cells with weak membranous\\nlabeling:10% \\n\\n\\nUICC (and pending AJCC) Staging Summary \\nEdition: 8th \\npT2a\\npN0\\n\\n\"\n",
    "doc = nlp(text)\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    [{\"TEXT\": {\"REGEX\": r\"PD\"}}, {\"ORTH\": \"-\"}, {\"TEXT\": {\"REGEX\": r\"L1\"}}, {\"ORTH\": \":\"}],\n",
    "    [{\"TEXT\": {\"REGEX\": r\"PD\"}}, {\"ORTH\": \"-\"}, {\"TEXT\": {\"REGEX\": r\"L1\"}}, {\"ORTH\": \"(\"}],\n",
    "    [{\"TEXT\": {\"REGEX\": r\"PD\"}}, {\"ORTH\": \"-\"}, {\"TEXT\": {\"REGEX\": r\"L1\"}}],\n",
    "    [{\"TEXT\": {\"REGEX\": r\"PDL1\"}}],\n",
    "    [{\"TEXT\": {\"REGEX\": r\"PDL\"}}, {\"ORTH\": \"-\"}, {\"TEXT\": {\"REGEX\": r\"1\"}}],\n",
    "]\n",
    "\n",
    "# pattern = [\n",
    "#     [{\"TEXT\": {\"REGEX\": r\"PD[-]*L[-]*1[ ]*[:(]*\"}}]\n",
    "# ]\n",
    "\n",
    "\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "# pattern = [{\"REGEX\": \"PD-L1 \\(\"},\n",
    "#            {\"REGEX\": \"PD-L1:\"},\n",
    "#            {\"REGEX\": \"PD-L1|PDL-1|PDL1\"}]\n",
    "\n",
    "matcher.add(\"pdl1\", pattern)\n",
    "\n",
    "i=0\n",
    "for sent in doc.sents:\n",
    "    print('---------')\n",
    "    print(i)\n",
    "    print(sent)\n",
    "    if matcher(sent):\n",
    "        print('=====')\n",
    "        print(sent.text)\n",
    "        \n",
    "    i += 1\n",
    "\n",
    "\n",
    "# matches = matcher(doc)\n",
    "# for match_id, start, end in matches:\n",
    "#     string_id = nlp.vocab.strings[match_id]\n",
    "#     span = doc[start:end]\n",
    "#     print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPDL1(report=rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\.?|nited)$\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"^[Ss](\\.?|tates)$\"}}]\n",
    "matcher.add(\"US\", [pattern])\n",
    "doc = nlp(u\"I'm from the united States.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(doc)\n",
    "    print(string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\"\"\"\n",
    "Graham Greene is his favorite author. He wrote his first book when he was a hundred and fifty years old.\n",
    "While writing this book, he had to fend off aliens and dinosaurs. Greene's second book might not have been written by him. \n",
    "Greene's cat in its deathbed testimony alleged that it was the original writer of the book. The fact that plot of the book revolves around \n",
    "rats conquering the world, lends credence to the idea that only a cat could have been the true writer of such an inane book.\"\"\")\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\": \"write\"},{\"OP\": \"*\"},{\"LEMMA\": \"book\"}]\n",
    "matcher.add(\"testy\", [pattern])\n",
    "\n",
    "print(\"----- Using Matcher -----\")\n",
    "for sent in doc.sents:\n",
    "    if matcher(sent):\n",
    "        print(sent.text)\n",
    "\n",
    "print(\"----- Using Dependency Matcher -----\")\n",
    "\n",
    "deppattern = [\n",
    "        {\"RIGHT_ID\": \"wrote\", \"RIGHT_ATTRS\": {\"LEMMA\": \"write\"}},\n",
    "        {\"LEFT_ID\": \"wrote\", \"REL_OP\": \">\", \"RIGHT_ID\": \"book\", \n",
    "            \"RIGHT_ATTRS\": {\"LEMMA\": \"book\"}}\n",
    "        ]\n",
    "\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "dmatcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "dmatcher.add(\"BOOK\", [deppattern])\n",
    "\n",
    "for _, (start, end) in dmatcher(doc):\n",
    "    print(doc[start].sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'radiology/ddp_radiology_reports.tsv'\n",
    "fname_minio_env = '/mind_data/fongc2/cdm-utilities/minio_env.txt'\n",
    "obj_minio = MinioAPI(fname_minio_env=fname_minio_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = obj_minio.load_obj(path_object=fname)\n",
    "df_rad = pd.read_csv(obj, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad.loc[df_rad['PROCEDURE_TYPE'] == 'Submitted Films', 'RADIOLOGY_PROCEDURE_NAME'].value_counts().reset_index().to_csv('radiology_proc_names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['PROCEDURE_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad.loc[df_rad['PROCEDURE_TYPE'] == 'XR'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_proc = [['XR'], ['IR'], ['NM'], ['BI', 'MAMMO'], ['CT'], ['MR', 'Volumetrics'], ['PET'], ['US', 'VASC']]\n",
    "list_proc_norm = ['XR', 'IR', 'NM', 'BI', 'CT', 'MR', 'PET', 'US']\n",
    "df_rad['PROCEDURE_TYPE_CLEAN'] = df_rad['PROCEDURE_TYPE']\n",
    "for i,current_list in enumerate(list_proc):\n",
    "    logic_proc = df_rad['PROCEDURE_TYPE'].str.upper().str.contains('|'.join(list_proc[i])) | df_rad['RADIOLOGY_PROCEDURE_NAME'].str.upper().str.contains('|'.join(list_proc[i]))\n",
    "    df_rad.loc[logic_proc, 'PROCEDURE_TYPE_CLEAN'] = list_proc_norm[i]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = obj_minio.load_obj(path_object='radiology/radiology_clean_annotations.tsv')\n",
    "df_rad_anno = pd.read_csv(obj, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad_anno['PROCEDURE_TYPE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad['PROCEDURE_TYPE_CLEAN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rad[df_rad['PROCEDURE_TYPE_CLEAN'] == 'Submitted Films'].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_rad.loc[df_rad['PROCEDURE_TYPE_CLEAN'] == 'Submitted Films'].sample()['RRPT_REPORT_TXT'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cdm",
   "language": "python",
   "name": "env_cdm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
